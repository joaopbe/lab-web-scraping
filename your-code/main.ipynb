{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Web Scraping Lab\n",
    "\n",
    "You will find in this notebook some scrapy exercises to practise your scraping skills.\n",
    "\n",
    "**Tips:**\n",
    "\n",
    "- Check the response status code for each request to ensure you have obtained the intended content.\n",
    "- Print the response text in each request to understand the kind of info you are getting and its format.\n",
    "- Check for patterns in the response text to extract the data/info requested in each question.\n",
    "- Visit the urls below and take a look at their source code through Chrome DevTools. You'll need to identify the html tags, special class names, etc used in the html content you are expected to extract.\n",
    "\n",
    "**Resources**:\n",
    "- [Requests library](http://docs.python-requests.org/en/master/#the-user-guide)\n",
    "- [Beautiful Soup Doc](https://www.crummy.com/software/BeautifulSoup/bs4/doc/)\n",
    "- [Urllib](https://docs.python.org/3/library/urllib.html#module-urllib)\n",
    "- [re lib](https://docs.python.org/3/library/re.html)\n",
    "- [lxml lib](https://lxml.de/)\n",
    "- [Scrapy](https://scrapy.org/)\n",
    "- [List of HTTP status codes](https://en.wikipedia.org/wiki/List_of_HTTP_status_codes)\n",
    "- [HTML basics](http://www.simplehtmlguide.com/cheatsheet.php)\n",
    "- [CSS basics](https://www.cssbasics.com/#page_start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Below are the libraries and modules you may need. `requests`,  `BeautifulSoup` and `pandas` are already imported for you. If you prefer to use additional libraries feel free to do it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Download, parse (using BeautifulSoup), and print the content from the Trending Developers page from GitHub:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise\n",
    "url = 'https://github.com/trending/developers'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "html = requests.get(url).content\n",
    "soup = BeautifulSoup(html, \"lxml\")  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Display the names of the trending developers retrieved in the previous step.\n",
    "\n",
    "Your output should be a Python list of developer names. Each name should not contain any html tag.\n",
    "\n",
    "**Instructions:**\n",
    "\n",
    "1. Find out the html tag and class names used for the developer names. You can achieve this using Chrome DevTools.\n",
    "\n",
    "1. Use BeautifulSoup to extract all the html elements that contain the developer names.\n",
    "\n",
    "1. Use string manipulation techniques to replace whitespaces and linebreaks (i.e. `\\n`) in the *text* of each html element. Use a list to store the clean names.\n",
    "\n",
    "1. Print the list of names.\n",
    "\n",
    "Your output should look like below:\n",
    "\n",
    "```\n",
    "['trimstray (@trimstray)',\n",
    " 'joewalnes (JoeWalnes)',\n",
    " 'charlax (Charles-AxelDein)',\n",
    " 'ForrestKnight (ForrestKnight)',\n",
    " 'revery-ui (revery-ui)',\n",
    " 'alibaba (Alibaba)',\n",
    " 'Microsoft (Microsoft)',\n",
    " 'github (GitHub)',\n",
    " 'facebook (Facebook)',\n",
    " 'boazsegev (Bo)',\n",
    " 'google (Google)',\n",
    " 'cloudfetch',\n",
    " 'sindresorhus (SindreSorhus)',\n",
    " 'tensorflow',\n",
    " 'apache (TheApacheSoftwareFoundation)',\n",
    " 'DevonCrawford (DevonCrawford)',\n",
    " 'ARMmbed (ArmMbed)',\n",
    " 'vuejs (vuejs)',\n",
    " 'fastai (fast.ai)',\n",
    " 'QiShaoXuan (Qi)',\n",
    " 'joelparkerhenderson (JoelParkerHenderson)',\n",
    " 'torvalds (LinusTorvalds)',\n",
    " 'CyC2018',\n",
    " 'komeiji-satori (神楽坂覚々)',\n",
    " 'script-8']\n",
    " ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scrap = soup.find_all('h1', attrs = {'class':'h3 lh-condensed'})\n",
    "\n",
    "list_raw =[element.text for element in scrap]\n",
    "final = [el.strip() for el in list_raw]\n",
    "final\n",
    "\n",
    "scrap2 = soup.find_all('p', attrs = {'class':'f4 text-normal mb-1'})\n",
    "final2 = [el.text.strip() for el in scrap2]\n",
    "final2\n",
    "\n",
    "final_result = [el[0] + \" \" + \"(\" + el[1] + \")\" for el in (zip(final,final2))]\n",
    "final_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Display the trending Python repositories in GitHub.\n",
    "\n",
    "The steps to solve this problem is similar to the previous one except that you need to find out the repository names instead of developer names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise\n",
    "url = 'https://github.com/trending/python?since=daily'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "html = requests.get(url).content\n",
    "soup = BeautifulSoup(html, \"lxml\")  \n",
    "\n",
    "scrap = soup.find_all('h1', attrs = {'class':'h3 lh-condensed'})\n",
    "\n",
    "list_raw =[element.text for element in scrap]\n",
    "\n",
    "final = [el.strip().replace('\\n','').replace(' ','').replace('/',\" \") for el in list_raw]\n",
    "final"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Display all the image links from Walt Disney wikipedia page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise\n",
    "url = 'https://en.wikipedia.org/wiki/Walt_Disney'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "html = requests.get(url).content\n",
    "soup = BeautifulSoup(html, \"lxml\")  \n",
    "\n",
    "scrap = soup.find_all('img')\n",
    "\n",
    "wd_images =[element.get('src') for element in scrap]\n",
    "wd_images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Find the number of titles that have changed in the United States Code since its last release point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise\n",
    "url = 'http://uscode.house.gov/download/download.shtml'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "html = requests.get(url).content\n",
    "soup = BeautifulSoup(html, \"lxml\")  \n",
    "\n",
    "scrap = soup.find_all('div', attrs = {'class':'usctitlechanged'})\n",
    "\n",
    "list_raw =[element.text for element in scrap]\n",
    "\n",
    "changed = [el.strip() for el in list_raw]\n",
    "changed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Find a Python list with the top ten FBI's Most Wanted names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise\n",
    "url = 'https://www.fbi.gov/wanted/topten'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "html = requests.get(url).content\n",
    "soup = BeautifulSoup(html, \"lxml\")  \n",
    "\n",
    "scrap = soup.find_all('h3', attrs = {'class':'title'})\n",
    "\n",
    "list_raw =[element.text for element in scrap]\n",
    "\n",
    "fbi10 = [el.strip() for el in list_raw]\n",
    "fbi10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Count the number of tweets by a given Twitter account.\n",
    "Ask the user for the handle (@handle) of a twitter account. You will need to include a ***try/except block*** for account names not found. \n",
    "<br>***Hint:*** the program should count the number of tweets for any provided account."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise \n",
    "# You will need to add the account credentials to this url\n",
    "url = 'https://twitter.com/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "account = input('Input your account name on Twitter: ')\n",
    "response = requests.get('https://twitter.com/'+account).content\n",
    "soup = BeautifulSoup(response,'lxml')\n",
    "\n",
    "try:\n",
    "    tweet_0 = soup.find('li',{'class':'ProfileNav-item ProfileNav-item--tweets is-active'})\n",
    "    tweets= tweet_0.find('span',{'class':'ProfileNav-value'}).get('data-count')\n",
    "    print(\"%s tweeted %s tweets.\" %(account,tweets))\n",
    "\n",
    "except:\n",
    "    print('That account does not exist!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Number of followers of a given twitter account\n",
    "Ask the user for the handle (@handle) of a twitter account. You will need to include a ***try/except block*** for account names not found. \n",
    "<br>***Hint:*** the program should count the followers for any provided account."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise \n",
    "# You will need to add the account credentials to this url\n",
    "url = 'https://twitter.com/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "account = input('Input your account name on Twitter: ')\n",
    "response = requests.get('https://twitter.com/'+account).content\n",
    "soup = BeautifulSoup(response,'lxml')\n",
    "\n",
    "try:\n",
    "    tweet_0 = soup.find('li',{'class':'ProfileNav-item ProfileNav-item--followers'})\n",
    "    followers= tweet_0.find('span',{'class':'ProfileNav-value'}).get('data-count')\n",
    "    print(\"%s has %s twitter followers.\" %(account,followers))\n",
    "\n",
    "except:\n",
    "    print('That account does not exist!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### List all language names and number of related articles in the order they appear in wikipedia.org."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise\n",
    "url = 'https://www.wikipedia.org/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "html = requests.get(url).content\n",
    "soup = BeautifulSoup(html, \"lxml\")  \n",
    "\n",
    "scrap = soup.find_all('strong')\n",
    "\n",
    "list_raw =[element.text for element in scrap]\n",
    "languages = list_raw [1:-1]\n",
    "languages\n",
    "\n",
    "scrap2 = soup.find_all('bdi', attrs = {'dir':'ltr'})\n",
    "list2_raw = [el.text for el in scrap2]\n",
    "articles = [el.replace(\"\\xa0\",\"\") for el in list2_raw [:-5]]\n",
    "articles\n",
    "\n",
    "final_result = [el[0] + \"; \" +  el[1] for el in (zip(languages,articles))]\n",
    "final_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A list with the different kind of datasets available in data.gov.uk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise\n",
    "url = 'https://data.gov.uk/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "html = requests.get(url).content\n",
    "soup = BeautifulSoup(html, \"lxml\") \n",
    "\n",
    "raw = soup.find_all('h2')\n",
    "\n",
    "datasets = [el.text for el in raw]\n",
    "datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Display the top 10 languages by number of native speakers stored in a pandas dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise\n",
    "url = 'https://en.wikipedia.org/wiki/List_of_languages_by_number_of_native_speakers'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "html = requests.get(url).content\n",
    "soup = BeautifulSoup(html, \"lxml\")  \n",
    "\n",
    "# get the table\n",
    "table = soup.find_all('table', attrs = {'class':'wikitable sortable'})[1]\n",
    "\n",
    "#get the rows from the table\n",
    "rows = table.find_all('tr') \n",
    "\n",
    "#get and clean the text \n",
    "rows = [re.sub(r'[\\n]+',';',row.text).split(';') for row in rows[0:11]]\n",
    "\n",
    "#create the dataframe\n",
    "df = pd.DataFrame(rows2[1:], columns = rows2[0])\n",
    "df.set_index('Rank', inplace = True)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scrape a certain number of tweets of a given Twitter account."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise \n",
    "# You will need to add the account credentials to this url\n",
    "url = 'https://twitter.com/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "account = input('\\nInput your account name on Twitter: \\n')\n",
    "nb_tw = int(input('\\nHow many tweets you want to scrape? (1-20) \\n'))\n",
    "\n",
    "response = requests.get('https://twitter.com/'+account).content\n",
    "soup = BeautifulSoup(response,'lxml')\n",
    "\n",
    "# gets the tweets\n",
    "tweets = soup.find_all(\"li\", attrs={\"class\":\"js-stream-item\"})\n",
    "\n",
    "# prints the requested nb of tweets\n",
    "print ('\\nThe tweets are: ... \\n')\n",
    "for tweet in tweets[0:nb_tw]:\n",
    "    if tweet.find('p',{\"class\":'tweet-text'}):\n",
    "      print(tweet.find('p',{\"class\":'tweet-text'}).text.strip(),'\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Display IMDB's top 250 data (movie name, initial release, director name and stars) as a pandas dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise \n",
    "url = 'https://www.imdb.com/chart/top'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "html = requests.get(url).content\n",
    "soup = BeautifulSoup(html, \"lxml\")  \n",
    "\n",
    "# get the table\n",
    "table = soup.find_all('table', attrs = {'data-caller-name': 'chart-top250movie'}) [0]\n",
    "table\n",
    "\n",
    "#get rows of table\n",
    "rows = table.find_all('tr') \n",
    "\n",
    "#get director and stars\n",
    "cast_info = [el.get('title').split(' (dir.), ') for el in table.find_all('a') if el.get('title') is not None]\n",
    "director = [el[0] for el in cast_info]\n",
    "stars = [el[1] for el in cast_info]\n",
    "\n",
    "#get and clean the text \n",
    "rows = [re.sub(r'[\\n]+',';',row.text).strip(r'[; ]*').split(';') for row in rows] [1:]\n",
    "for el in rows: el[2] = el[2].strip(r'[()]*')\n",
    "\n",
    "#dataframe \n",
    "df = pd.DataFrame(rows)\n",
    "df['Director'] = director\n",
    "df['Stars'] = stars\n",
    "df.drop(columns = [4,5,6,7], inplace = True)\n",
    "df.columns = ['Rank', 'Name', 'Year', 'IMDB Rating', 'Director', 'Stars']\n",
    "df.set_index('Rank', inplace = True)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Display the movie name, year and a brief summary of the top 10 random movies (IMDB) as a pandas dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This is the url you will scrape in this exercise\n",
    "url = 'http://www.imdb.com/chart/top'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "html = requests.get(url).content\n",
    "soup = BeautifulSoup(html, \"lxml\")  \n",
    "\n",
    "# get the table\n",
    "table = soup.find_all('table', attrs = {'data-caller-name': 'chart-top250movie'}) [0]\n",
    "table\n",
    "\n",
    "#get the links for the movies in the TOP 10\n",
    "url_basic = 'http://www.imdb.com'\n",
    "movie_link = [url_basic  + el.get('href') for el in table.find_all('a') [0:20]] [::2]\n",
    "\n",
    "#scrap each link for name, year and summary\n",
    "name_year_sum=[]\n",
    "for link in movie_link:\n",
    "    html = requests.get(link).content\n",
    "    soup = BeautifulSoup(html, \"lxml\")  \n",
    "    scrap = soup.find_all('h1') [0].text.strip().split('\\xa0') + [soup.find_all('div', attrs = {'class':'summary_text'}) [0].text.strip()]\n",
    "    name_year_sum.append (scrap)\n",
    "\n",
    "#remove () from year\n",
    "for el in name_year_sum: el[1] = el[1].strip(r'[()]*')    \n",
    " \n",
    "#dataframe\n",
    "df = pd.DataFrame(name_year_sum)\n",
    "df.columns = ['Name', 'Year', 'Summary']\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bonus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Find the live weather report (temperature, wind speed, description and weather) of a given city."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://openweathermap.org/current\n",
    "city = input('Enter the city: ')\n",
    "url = 'http://api.openweathermap.org/data/2.5/weather?'+'q='+city+'&APPID=b35975e18dc93725acb092f7272cc6b8&units=metric'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "\n",
    "html = requests.get(url).content\n",
    "soup = BeautifulSoup(html, \"lxml\") \n",
    "\n",
    "#get all the info as a dic\n",
    "report = ast.literal_eval(soup.find_all('body')[0].text)\n",
    "\n",
    "#get temperature\n",
    "temp = report['main']['temp']\n",
    "wind_speed = report['wind']['speed']\n",
    "description = report ['weather'][0]['description']\n",
    "weather = report ['weather'][0]['main']\n",
    "\n",
    "print('For %s the temperature is %1.1f ºC, the wind speed is %1.1f ms, the weather is %s with %s' %(city, temp, wind_speed, description, weather))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Find the book name, price and stock availability as a pandas dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise. \n",
    "# It is a fictional bookstore created to be scraped. \n",
    "url = 'http://books.toscrape.com/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "html = requests.get(url).content\n",
    "soup = BeautifulSoup(html, \"lxml\")  \n",
    "\n",
    "info = []\n",
    "for num in range(1,51):\n",
    "    book_link = url + '/catalogue/page-' + str(num) + '.html'\n",
    "    html = requests.get(book_link).content\n",
    "    soup = BeautifulSoup(html, \"lxml\")  \n",
    "    prices = [el.text for el in soup.find_all('p', class_= 'price_color')]\n",
    "    names = [el.get('title') for el in soup.find_all('a', href = True, title = True)]\n",
    "    stock = [el.text.strip() for el in soup.find_all('p', class_= 'instock availability')]\n",
    "    info += [(n,p,s) for n,p,s in zip(names,prices,stock)] \n",
    "\n",
    "df = pd.DataFrame(info)\n",
    "df.columns = ['Title', 'Price', 'Stock Availability']\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Display the 20 latest earthquakes info (date, time, latitude, longitude and region name) by the EMSC as a pandas dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise\n",
    "url = 'https://www.emsc-csem.org/Earthquake/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "html = requests.get(url).content\n",
    "soup = BeautifulSoup(html, \"lxml\")  \n",
    "\n",
    "# get the table\n",
    "table = soup.find_all('table') [3]\n",
    "\n",
    "#Date_Time Extraction\n",
    "date_time_raw = table.find_all('a', href=True, onmouseout=False)\n",
    "date_time = []\n",
    "\n",
    "href_str = '/Earthquake/earthquake.php?id='\n",
    "for el in date_time_raw:\n",
    "    if href_str in el.get('href'):\n",
    "        date_time.append(el.text.replace('\\xa0',' ').split(' '))\n",
    "\n",
    "#Date\n",
    "date = [el[0] for el in date_time]\n",
    "\n",
    "#Time\n",
    "time = [el[-1] for el in date_time] \n",
    "    \n",
    "#Region Extraction\n",
    "region = [el.text.strip('\\xa0') for el in table.find_all('td', class_ = 'tb_region')]\n",
    "region\n",
    "\n",
    "#Coordinate extractiom\n",
    "#Extraction of E/W S/N\n",
    "orient = table.find_all('td', class_ = 'tabev2')\n",
    "ori_clean= [orient[i].text.strip('\\xa0') for i in range(len(orient)) if (i % 3 != 2)]\n",
    "ori_lon = [ori_clean[i] for i in range(len(ori_clean)) if i % 2 ]\n",
    "ori_lat = [ori_clean[i] for i in range(len(ori_clean)) if not i % 2 ]\n",
    "\n",
    "#Extraction of Lat Long Num.\n",
    "coor = [item.text.strip('\\xa0') for item in table.find_all('td', class_ = 'tabev1')]\n",
    "lon = [coor[i] for i in range(len(coor)) if i % 2]\n",
    "lat = [coor[i] for i in range(len(coor)) if not i % 2]\n",
    "\n",
    "#Full Lat/Lon\n",
    "full_lat = [i+j for i,j in zip(lat,ori_lat)]\n",
    "full_lon = [i+j for i,j in zip(lon,ori_lon)]\n",
    "\n",
    "#create the dataframe\n",
    "dict = {'date':date,'time':time,'latitude':full_lat,'longitude':full_lon,'region':region}\n",
    "\n",
    "df = pd.DataFrame(dict)[0:20]\n",
    "df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
